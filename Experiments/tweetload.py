import requests
import os

# Set up the base parameters
url = "http://localhost:19002/query/service"
headers = {
    "Content-Type": "application/x-www-form-urlencoded"
}

# Drop if exists and create dataverse and dataset
def setup_dataverse_and_dataset():
    # Define the query to setup the dataverse and dataset
    query = """
    drop dataverse tweetverse if exists;
    create dataverse tweetverse;
    use tweetverse;
    create type Opentype as {uid:uuid};
    create dataset tweetsdataset(Opentype) primary key uid autogenerated with {'storage-block-compression': {'scheme': 'none'}};
    """

    # Data for the query
    data = {
        "statement": query,
        "pretty": "true",
        "client_context_id": "setup123"
    }

    # Send the request
    response = requests.post(url, headers=headers, data=data)
    print("Setup response:", response.text)

# Function to load individual JSON files into the dataset
def load_json_files(directory):
    # Iterate through all json files in the directory
    for filename in sorted(os.listdir(directory)):
        if filename.endswith(".json"):
            file_path = os.path.join(directory, filename)
            print(file_path)
            # Define the load query
            load_query = "use tweetverse;copy tweetsdataset from localfs path ('localhost://"
            load_query+=file_path
            load_query+="') with {'format':'json'};"

            # Data for the load query
            data = {
                "statement": load_query,
                "pretty": "true",
                "client_context_id": f"load_{filename}"
            }

            # Send the request to load the file
            response = requests.post(url, headers=headers, data=data)
            print(f"Loaded {filename} response:", response.text)

# Main execution
if __name__ == "__main__":
    setup_dataverse_and_dataset()
    tweet_dump_directory = "/home/dbis-nuc10/DBIS/data/tweet_dump"  # Update this path to your actual directory
    load_json_files(tweet_dump_directory)
